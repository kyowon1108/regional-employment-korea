import pandas as pd
import numpy as np

def validate_preprocessed_data():
    """ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦"""
    print("=== ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦ ===\\n")

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv("/Users/kapr/Desktop/DataAnalyze/new_analysis/data/new_processed/comprehensive_integrated_data.csv")
    summary_df = pd.read_csv("/Users/kapr/Desktop/DataAnalyze/new_analysis/data/new_processed/comprehensive_summary.csv")

    print(f"ğŸ“Š íŒ¨ë„ ë°ì´í„°: {df.shape}")
    print(f"ğŸ“‹ ìš”ì•½ ë°ì´í„°: {summary_df.shape}")

    # 1. ê¸°ë³¸ í˜„í™©
    print("\\n1. ê¸°ë³¸ í˜„í™©")
    print(f"   â€¢ ì§€ìì²´ ìˆ˜: {df[['ì‹œë„', 'ì‹œêµ°êµ¬']].drop_duplicates().shape[0]}ê°œ")
    print(f"   â€¢ ë¶„ì„ ê¸°ê°„: {df['ì—°ë„'].min()}-{df['ì—°ë„'].max()}ë…„")
    print(f"   â€¢ ì‹œë„ ìˆ˜: {df['ì‹œë„'].nunique()}ê°œ")

    # 2. ì‹œë„ë³„ ë¶„í¬
    print("\\n2. ì‹œë„ë³„ ì§€ìì²´ ë¶„í¬")
    sido_counts = df.groupby('ì‹œë„')['ì‹œêµ°êµ¬'].nunique().sort_values(ascending=False)
    for sido, count in sido_counts.items():
        sido_short = sido.replace('íŠ¹ë³„ìì¹˜ë„', '').replace('ê´‘ì—­ì‹œ', '').replace('íŠ¹ë³„ì‹œ', '').replace('ë„', '')
        print(f"   â€¢ {sido_short}: {count}ê°œ")

    # 3. E9 ì²´ë¥˜ì í˜„í™©
    print("\\n3. E9 ì²´ë¥˜ì í˜„í™© (5ë…„ í‰ê·  ê¸°ì¤€)")
    e9_stats = summary_df['E9_ì²´ë¥˜ììˆ˜']
    print(f"   â€¢ í‰ê· : {e9_stats.mean():.0f}ëª…")
    print(f"   â€¢ ìµœëŒ€: {e9_stats.max():.0f}ëª…")
    print(f"   â€¢ E9 > 0ì¸ ì§€ì—­: {len(summary_df[summary_df['E9_ì²´ë¥˜ììˆ˜'] > 0])}ê°œ")

    # E9 ìƒìœ„ 10ê°œ ì§€ì—­
    print("\\n   ğŸ“ E9 ì²´ë¥˜ììˆ˜ ìƒìœ„ 10ê°œ ì§€ìì²´:")
    top10_e9 = summary_df.nlargest(10, 'E9_ì²´ë¥˜ììˆ˜')[['ì‹œë„', 'ì‹œêµ°êµ¬', 'E9_ì²´ë¥˜ììˆ˜', 'ê³ ìš©ë¥ ']]
    for i, (_, row) in enumerate(top10_e9.iterrows(), 1):
        sido_short = row['ì‹œë„'].replace('íŠ¹ë³„ìì¹˜ë„', '').replace('ê´‘ì—­ì‹œ', '').replace('íŠ¹ë³„ì‹œ', '').replace('ë„', '')
        print(f"   {i:2d}. {sido_short} {row['ì‹œêµ°êµ¬']:8s}: {row['E9_ì²´ë¥˜ììˆ˜']:6.0f}ëª… (ê³ ìš©ë¥ : {row['ê³ ìš©ë¥ ']:.1f}%)")

    # 4. ê³ ìš©ë¥  í˜„í™©
    print("\\n4. ê³ ìš©ë¥  í˜„í™© (5ë…„ í‰ê·  ê¸°ì¤€)")
    emp_stats = summary_df['ê³ ìš©ë¥ ']
    print(f"   â€¢ í‰ê· : {emp_stats.mean():.1f}%")
    print(f"   â€¢ ìµœê³ : {emp_stats.max():.1f}% ({summary_df.loc[summary_df['ê³ ìš©ë¥ '].idxmax(), 'ì‹œêµ°êµ¬']})")
    print(f"   â€¢ ìµœì €: {emp_stats.min():.1f}% ({summary_df.loc[summary_df['ê³ ìš©ë¥ '].idxmin(), 'ì‹œêµ°êµ¬']})")

    # ê³ ìš©ë¥  ìƒìœ„ 10ê°œ ì§€ì—­
    print("\\n   ğŸ“ ê³ ìš©ë¥  ìƒìœ„ 10ê°œ ì§€ìì²´:")
    top10_emp = summary_df.nlargest(10, 'ê³ ìš©ë¥ ')[['ì‹œë„', 'ì‹œêµ°êµ¬', 'ê³ ìš©ë¥ ', 'E9_ì²´ë¥˜ììˆ˜']]
    for i, (_, row) in enumerate(top10_emp.iterrows(), 1):
        sido_short = row['ì‹œë„'].replace('íŠ¹ë³„ìì¹˜ë„', '').replace('ê´‘ì—­ì‹œ', '').replace('íŠ¹ë³„ì‹œ', '').replace('ë„', '')
        print(f"   {i:2d}. {sido_short} {row['ì‹œêµ°êµ¬']:8s}: {row['ê³ ìš©ë¥ ']:5.1f}% (E9: {row['E9_ì²´ë¥˜ììˆ˜']:4.0f}ëª…)")

    # 5. ìƒê´€ê´€ê³„ í™•ì¸
    print("\\n5. ì£¼ìš” ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„")
    corr_vars = ['ê³ ìš©ë¥ ', 'E9_ì²´ë¥˜ììˆ˜', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘']
    corr_matrix = summary_df[corr_vars].corr()

    print(f"   â€¢ ê³ ìš©ë¥  âŸ· E9 ì²´ë¥˜ììˆ˜: {corr_matrix.loc['ê³ ìš©ë¥ ', 'E9_ì²´ë¥˜ììˆ˜']:.3f}")
    print(f"   â€¢ ê³ ìš©ë¥  âŸ· ì œì¡°ì—…ë¹„ì¤‘:  {corr_matrix.loc['ê³ ìš©ë¥ ', 'ì œì¡°ì—…_ë¹„ì¤‘']:.3f}")
    print(f"   â€¢ ê³ ìš©ë¥  âŸ· ì„œë¹„ìŠ¤ì—…ë¹„ì¤‘: {corr_matrix.loc['ê³ ìš©ë¥ ', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘']:.3f}")

    # 6. ì—°ë„ë³„ ì¶”ì´
    print("\\n6. ì—°ë„ë³„ í‰ê·  ì¶”ì´")
    yearly_stats = df.groupby('ì—°ë„').agg({
        'ê³ ìš©ë¥ ': 'mean',
        'E9_ì²´ë¥˜ììˆ˜': 'mean',
        'ì œì¡°ì—…_ë¹„ì¤‘': 'mean'
    })

    print("   ì—°ë„    ê³ ìš©ë¥     E9í‰ê·    ì œì¡°ì—…ë¹„ì¤‘")
    print("   " + "-"*35)
    for year, row in yearly_stats.iterrows():
        print(f"   {year}   {row['ê³ ìš©ë¥ ']:5.1f}%   {row['E9_ì²´ë¥˜ììˆ˜']:6.0f}ëª…   {row['ì œì¡°ì—…_ë¹„ì¤‘']:5.1f}%")

    # 7. ë°ì´í„° ì™„ì „ì„± í™•ì¸
    print("\\n7. ë°ì´í„° ì™„ì „ì„± í™•ì¸")

    # ê° ì§€ìì²´ë³„ ì—°ë„ ìˆ˜ í™•ì¸
    completeness = df.groupby(['ì‹œë„', 'ì‹œêµ°êµ¬']).size()
    incomplete = completeness[completeness != 5]  # 5ë…„ì´ ì•„ë‹Œ ì§€ì—­

    if len(incomplete) > 0:
        print(f"   âš ï¸ ë¶ˆì™„ì „í•œ ë°ì´í„° ì§€ì—­: {len(incomplete)}ê°œ")
        for (sido, sigungu), count in incomplete.head().items():
            print(f"      - {sido} {sigungu}: {count}ë…„")
    else:
        print("   âœ… ëª¨ë“  ì§€ìì²´ê°€ 5ë…„ ì™„ì „ ë°ì´í„° ë³´ìœ ")

    # 8. ê²°ì¸¡ê°’ í™•ì¸
    print("\\n8. ê²°ì¸¡ê°’ í˜„í™©")
    missing_info = df.isnull().sum()
    total_missing = missing_info.sum()

    if total_missing == 0:
        print("   âœ… ê²°ì¸¡ê°’ ì—†ìŒ")
    else:
        print(f"   âš ï¸ ì´ {total_missing}ê°œ ê²°ì¸¡ê°’:")
        for col, count in missing_info.items():
            if count > 0:
                print(f"      - {col}: {count}ê°œ")

    print("\\n" + "="*50)
    print("ğŸ“‹ ì „ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ: âœ… ì–‘í˜¸")
    print(f"ğŸ“Š ìµœì¢… ë¶„ì„ ì¤€ë¹„ ë°ì´í„°: {len(summary_df)}ê°œ ì§€ìì²´ Ã— 5ë…„ = {len(df)}ê°œ ê´€ì¸¡ì¹˜")
    print("="*50)

    return df, summary_df

def identify_potential_issues():
    """ì ì¬ì  ë¬¸ì œì  ì‹ë³„"""
    print("\\n=== ì ì¬ì  ì´ìŠˆ í™•ì¸ ===")

    df = pd.read_csv("/Users/kapr/Desktop/DataAnalyze/new_analysis/data/new_processed/comprehensive_integrated_data.csv")
    summary_df = pd.read_csv("/Users/kapr/Desktop/DataAnalyze/new_analysis/data/new_processed/comprehensive_summary.csv")

    issues = []

    # 1. ê·¹ê°’ í™•ì¸
    emp_q1, emp_q3 = summary_df['ê³ ìš©ë¥ '].quantile([0.25, 0.75])
    emp_iqr = emp_q3 - emp_q1
    emp_outliers = summary_df[
        (summary_df['ê³ ìš©ë¥ '] < emp_q1 - 1.5*emp_iqr) |
        (summary_df['ê³ ìš©ë¥ '] > emp_q3 + 1.5*emp_iqr)
    ]

    if len(emp_outliers) > 0:
        issues.append(f"ê³ ìš©ë¥  ê·¹ê°’: {len(emp_outliers)}ê°œ ì§€ì—­")
        print(f"ğŸ” ê³ ìš©ë¥  ê·¹ê°’ ì§€ì—­ ({len(emp_outliers)}ê°œ):")
        for _, row in emp_outliers[['ì‹œêµ°êµ¬', 'ê³ ìš©ë¥ ']].iterrows():
            print(f"   â€¢ {row['ì‹œêµ°êµ¬']}: {row['ê³ ìš©ë¥ ']:.1f}%")

    # 2. E9 ì§‘ì¤‘ë„ í™•ì¸
    e9_total = summary_df['E9_ì²´ë¥˜ììˆ˜'].sum()
    if e9_total > 0:
        top5_e9_sum = summary_df.nlargest(5, 'E9_ì²´ë¥˜ììˆ˜')['E9_ì²´ë¥˜ììˆ˜'].sum()
        concentration = top5_e9_sum / e9_total * 100

        if concentration > 80:
            issues.append(f"E9 ì§‘ì¤‘ë„ ë†’ìŒ: ìƒìœ„ 5ê°œ ì§€ì—­ì´ {concentration:.1f}% ì ìœ ")
            print(f"ğŸ” E9 ì§‘ì¤‘ë„: ìƒìœ„ 5ê°œ ì§€ì—­ì´ ì „ì²´ì˜ {concentration:.1f}% ì ìœ ")

    # 3. 0ê°’ ë¹„ìœ¨ í™•ì¸
    zero_e9 = len(summary_df[summary_df['E9_ì²´ë¥˜ììˆ˜'] == 0])
    zero_ratio = zero_e9 / len(summary_df) * 100

    if zero_ratio > 70:
        issues.append(f"E9 ì œë¡œ ì§€ì—­ ë§ìŒ: {zero_ratio:.1f}%")
        print(f"ğŸ” E9 ì²´ë¥˜ì 0ëª… ì§€ì—­: {zero_e9}ê°œ ({zero_ratio:.1f}%)")

    # 4. ì œì¡°ì—… ë¹„ì¤‘ í™•ì¸
    high_manufacturing = len(summary_df[summary_df['ì œì¡°ì—…_ë¹„ì¤‘'] > 60])
    if high_manufacturing > 0:
        print(f"ğŸ” ì œì¡°ì—… ê³ ì§‘ì¤‘ ì§€ì—­: {high_manufacturing}ê°œ (60% ì´ìƒ)")

    if len(issues) == 0:
        print("âœ… íŠ¹ë³„í•œ ì´ìŠˆ ì—†ìŒ")
    else:
        print(f"\\nâš ï¸ í™•ì¸ëœ ì´ìŠˆ: {len(issues)}ê°œ")
        for issue in issues:
            print(f"   â€¢ {issue}")

    print("\\nğŸ’¡ ì¶”ì²œ ì‚¬í•­:")
    print("   1. E9 ì§‘ì¤‘ í˜„ìƒìœ¼ë¡œ ì¸í•´ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œ ì£¼ì˜ í•„ìš”")
    print("   2. ê·¹ê°’ ì§€ì—­ì— ëŒ€í•œ ë³„ë„ ë¶„ì„ ê³ ë ¤")
    print("   3. ì œì¡°ì—…/ì„œë¹„ìŠ¤ì—… ë¹„ì¤‘ì„ í™œìš©í•œ ì‚°ì—…êµ¬ì¡° ë¶„ì„ ê°€ëŠ¥")

def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    df, summary_df = validate_preprocessed_data()
    identify_potential_issues()

    return df, summary_df

if __name__ == "__main__":
    df, summary_df = main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Employment Analysis: 2019-2023 Panel Study
========================================================
- Load integrated CSV with auto-detection of Korean column names
- Build balanced panel for municipalities (2019-2023)
- Run panel regression with fixed effects
- Generate choropleths for 5-year averages and single-year comparisons
- Fetch South Korea SGG boundaries automatically from web
"""

import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import requests
import warnings
import re
import os
import sys
from pathlib import Path
from io import StringIO
import zipfile

# Statistical modeling
try:
    from linearmodels.panel import PanelOLS
    from linearmodels import PooledOLS
except ImportError:
    print("Installing linearmodels...")
    os.system("pip install linearmodels")
    from linearmodels.panel import PanelOLS
    from linearmodels import PooledOLS

# Map classification
try:
    import mapclassify
except ImportError:
    print("Installing mapclassify...")
    os.system("pip install mapclassify")
    import mapclassify

warnings.filterwarnings('ignore')

# Configuration
CSV_FILE = "data/processed/ìµœì¢…_í†µí•©ë°ì´í„°_ìˆ˜ì •_utf-8.csv"
YEARS = [2019, 2020, 2021, 2022, 2023]

def setup_korean_font():
    """Setup Korean font for matplotlib"""
    try:
        # Try different Korean fonts
        korean_fonts = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Noto Sans CJK KR']

        for font_name in korean_fonts:
            try:
                plt.rcParams['font.family'] = font_name
                # Test if the font works
                fig, ax = plt.subplots(figsize=(1, 1))
                ax.text(0.5, 0.5, 'í•œê¸€í…ŒìŠ¤íŠ¸', fontsize=12)
                plt.close(fig)
                print(f"âœ“ Korean font set: {font_name}")
                return
            except:
                continue

        # Fallback
        plt.rcParams['font.family'] = 'DejaVu Sans'
        print("âš  Using fallback font - Korean text may not display properly")

    except Exception as e:
        print(f"Font setup warning: {e}")
        plt.rcParams['font.family'] = 'DejaVu Sans'

def create_directories():
    """Create output directory structure"""
    dirs = [
        'output/panel',
        'output/summary',
        'output/models',
        'output/maps',
        'output/debug',
        'output/cache'
    ]

    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

    print("âœ“ Output directories created")

def detect_columns(df):
    """Auto-detect column mapping for Korean column names"""
    columns = df.columns.tolist()
    print("Available columns:", columns)

    mapping = {}

    # Year detection
    year_patterns = ['ì—°ë„', 'ë…„ë„', 'year']
    for col in columns:
        if any(pattern in col.lower() for pattern in year_patterns):
            mapping['year'] = col
            break

    # Employment rate detection
    emp_patterns = ['ê³ ìš©ë¥ ', 'ê³ ìš© ìœ¨', 'ì·¨ì—…ë¥ ']
    for col in columns:
        if any(pattern in col for pattern in emp_patterns):
            mapping['employment'] = col
            break

    # E9 count detection
    e9_patterns = ['e-9', 'e9', 'ì™¸êµ­ì¸ê·¼ë¡œì']
    for col in columns:
        if any(pattern in col.lower().replace(' ', '').replace('_', '') for pattern in e9_patterns):
            mapping['e9_count'] = col
            break

    # SGG identification (ì‹œë„ + ì‹œêµ°êµ¬ combination)
    sido_patterns = ['ì‹œë„']
    sgg_patterns = ['ì‹œêµ°êµ¬']

    for col in columns:
        if any(pattern in col for pattern in sido_patterns):
            mapping['sido'] = col
        if any(pattern in col for pattern in sgg_patterns):
            mapping['sgg'] = col

    print("âœ“ Column mapping detected:")
    for key, value in mapping.items():
        print(f"  {key}: {value}")

    return mapping

def create_sgg_code(df, mapping):
    """Create standardized SGG code from ì‹œë„ and ì‹œêµ°êµ¬"""
    # Load Korean administrative code mapping
    sido_codes = {
        'ì„œìš¸íŠ¹ë³„ì‹œ': '11', 'ë¶€ì‚°ê´‘ì—­ì‹œ': '26', 'ëŒ€êµ¬ê´‘ì—­ì‹œ': '27', 'ì¸ì²œê´‘ì—­ì‹œ': '28',
        'ê´‘ì£¼ê´‘ì—­ì‹œ': '29', 'ëŒ€ì „ê´‘ì—­ì‹œ': '30', 'ìš¸ì‚°ê´‘ì—­ì‹œ': '31', 'ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ': '36',
        'ê²½ê¸°ë„': '41', 'ê°•ì›ë„': '42', 'ì¶©ì²­ë¶ë„': '43', 'ì¶©ì²­ë‚¨ë„': '44',
        'ì „ë¼ë¶ë„': '45', 'ì „ë¼ë‚¨ë„': '46', 'ê²½ìƒë¶ë„': '47', 'ê²½ìƒë‚¨ë„': '48', 'ì œì£¼íŠ¹ë³„ìì¹˜ë„': '50'
    }

    # Create a simple SGG code by concatenating sido code and a sequential number for each SGG
    df_copy = df.copy()

    # Create unique sido-sgg combinations
    sgg_combinations = df_copy[[mapping['sido'], mapping['sgg']]].drop_duplicates()
    sgg_combinations['sgg_code'] = ''

    for sido, sido_code in sido_codes.items():
        sido_mask = sgg_combinations[mapping['sido']] == sido
        sido_sggs = sgg_combinations[sido_mask][mapping['sgg']].unique()

        for i, sgg in enumerate(sorted(sido_sggs), 1):
            code = f"{sido_code}{i:03d}"  # e.g., 11001, 11002, etc.
            mask = (sgg_combinations[mapping['sido']] == sido) & (sgg_combinations[mapping['sgg']] == sgg)
            sgg_combinations.loc[mask, 'sgg_code'] = code

    # Merge back to original dataframe
    df_merged = df_copy.merge(
        sgg_combinations,
        on=[mapping['sido'], mapping['sgg']],
        how='left'
    )

    return df_merged

def load_and_prepare_data():
    """Load CSV and prepare panel data"""
    print("ğŸ“Š Loading and preparing data...")

    # Load CSV
    df = pd.read_csv(CSV_FILE, encoding='utf-8')
    print(f"âœ“ Loaded {len(df):,} rows from CSV")

    # Detect columns
    mapping = detect_columns(df)

    # Create SGG codes
    df = create_sgg_code(df, mapping)

    # Create SGG name for reference
    df['sgg_name'] = df[mapping['sido']] + ' ' + df[mapping['sgg']]

    # Filter to required years
    df = df[df[mapping['year']].isin(YEARS)].copy()
    print(f"âœ“ Filtered to years {YEARS}: {len(df):,} rows")

    # Aggregate by year and SGG (sum E9, average employment rate)
    agg_dict = {
        mapping['e9_count']: 'sum',
        mapping['employment']: 'mean',
        'sgg_name': 'first'
    }

    df_agg = df.groupby([mapping['year'], 'sgg_code']).agg(agg_dict).reset_index()
    df_agg.columns = ['year', 'sgg_code', 'e9_count', 'employment_rate', 'sgg_name']

    # Convert to numeric and drop missing
    df_agg['employment_rate'] = pd.to_numeric(df_agg['employment_rate'], errors='coerce')
    df_agg['e9_count'] = pd.to_numeric(df_agg['e9_count'], errors='coerce')

    # Drop rows with missing key variables
    before_drop = len(df_agg)
    df_agg = df_agg.dropna(subset=['employment_rate', 'e9_count'])
    print(f"âœ“ Dropped {before_drop - len(df_agg)} rows with missing data")

    # Create balanced panel
    year_counts = df_agg.groupby('sgg_code')['year'].count()
    balanced_sgg = year_counts[year_counts == len(YEARS)].index.tolist()

    df_balanced = df_agg[df_agg['sgg_code'].isin(balanced_sgg)].copy()

    print(f"âœ“ Balanced panel created: {len(balanced_sgg)} SGGs Ã— {len(YEARS)} years = {len(df_balanced):,} observations")

    if len(balanced_sgg) != 153:
        print(f"âš  Expected 153 SGGs, got {len(balanced_sgg)}")

    # Save panel data
    df_balanced.to_csv('output/panel/panel_balanced_2019_2023.csv', index=False, encoding='utf-8')

    # Save SGG list
    sgg_list = df_balanced[['sgg_code', 'sgg_name']].drop_duplicates().sort_values('sgg_name')
    sgg_list.to_csv('output/panel/sgg_list_153.csv', index=False, encoding='utf-8')

    return df_balanced, balanced_sgg

def fetch_boundaries():
    """Fetch South Korea SGG boundaries from web"""
    print("ğŸ—ºï¸ Fetching SGG boundaries...")

    cache_path = 'output/cache/sgg_boundary.geojson'

    # Try cached version first
    if os.path.exists(cache_path):
        print("âœ“ Loading cached boundaries")
        gdf = gpd.read_file(cache_path)
        return gdf

    try:
        # Fallback: southkorea-maps repository
        url = "https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2013/geojson/municipalities-geo.json"
        print(f"Downloading from: {url}")

        gdf = gpd.read_file(url)
        print(f"âœ“ Downloaded {len(gdf)} boundary polygons")

        # Ensure CRS is WGS84
        if gdf.crs != 'EPSG:4326':
            gdf = gdf.to_crs('EPSG:4326')

        # Standardize columns
        if 'code' in gdf.columns:
            gdf['sgg_code'] = gdf['code'].astype(str)
        elif 'SGG_CD' in gdf.columns:
            gdf['sgg_code'] = gdf['SGG_CD'].astype(str)

        # Keep essential columns
        essential_cols = ['sgg_code', 'geometry']
        if 'name' in gdf.columns:
            gdf['sgg_name_geo'] = gdf['name']
            essential_cols.append('sgg_name_geo')

        gdf = gdf[essential_cols].copy()

        # Save cache
        gdf.to_file(cache_path, driver='GeoJSON', encoding='utf-8')
        print(f"âœ“ Cached boundaries saved to {cache_path}")

        return gdf

    except Exception as e:
        print(f"âŒ Error fetching boundaries: {e}")
        print("Creating dummy boundaries for demonstration...")

        # Create simple dummy polygons for testing
        dummy_data = []
        for i in range(10):
            from shapely.geometry import Polygon
            poly = Polygon([(126+i*0.1, 37+i*0.1), (126.1+i*0.1, 37+i*0.1),
                          (126.1+i*0.1, 37.1+i*0.1), (126+i*0.1, 37.1+i*0.1)])
            dummy_data.append({'sgg_code': f"11{i:03d}", 'geometry': poly})

        gdf = gpd.GeoDataFrame(dummy_data, crs='EPSG:4326')
        return gdf

def calculate_averages(df_balanced):
    """Calculate 5-year averages by SGG"""
    print("ğŸ“ˆ Calculating 5-year averages...")

    avg_df = df_balanced.groupby('sgg_code').agg({
        'employment_rate': 'mean',
        'e9_count': 'mean',
        'sgg_name': 'first'
    }).reset_index()

    avg_df.columns = ['sgg_code', 'employment_rate_5yr_avg', 'e9_5yr_avg', 'sgg_name']

    # Save averages
    avg_df.to_csv('output/summary/avg_5yr_by_sgg.csv', index=False, encoding='utf-8')

    # Print summary statistics
    print("\nğŸ“Š 5-Year Average Summary Statistics:")
    print("Employment Rate (5yr avg):")
    print(f"  Min: {avg_df['employment_rate_5yr_avg'].min():.1f}%")
    print(f"  Median: {avg_df['employment_rate_5yr_avg'].median():.1f}%")
    print(f"  Max: {avg_df['employment_rate_5yr_avg'].max():.1f}%")

    print("\nE-9 Count (5yr avg):")
    print(f"  Min: {avg_df['e9_5yr_avg'].min():.0f}")
    print(f"  Median: {avg_df['e9_5yr_avg'].median():.0f}")
    print(f"  Max: {avg_df['e9_5yr_avg'].max():.0f}")

    # Correlation
    corr = np.corrcoef(avg_df['employment_rate_5yr_avg'],
                      np.log1p(avg_df['e9_5yr_avg']))[0,1]
    print(f"\nCorrelation (employment_rate vs log(1+e9)): {corr:.3f}")

    return avg_df

def run_panel_regression(df_balanced):
    """Run panel regression with fixed effects"""
    print("ğŸ”¬ Running panel regression...")

    # Prepare regression data
    reg_df = df_balanced.copy()
    reg_df['log_e9'] = np.log1p(reg_df['e9_count'])
    reg_df = reg_df.set_index(['sgg_code', 'year'])

    # Fixed Effects Model
    try:
        model_fe = PanelOLS(
            dependent=reg_df['employment_rate'],
            exog=reg_df[['log_e9']],
            entity_effects=True,
            time_effects=True
        )
        results_fe = model_fe.fit(cov_type='clustered', cluster_entity=True)

        # Save FE results
        with open('output/models/fe_regression.txt', 'w', encoding='utf-8') as f:
            f.write("Fixed Effects Panel Regression Results\n")
            f.write("=====================================\n\n")
            f.write("Model: employment_rate = Î² Ã— log(1 + e9_count) + Î±_i + Î³_t + Îµ\n")
            f.write("- Î±_i: Municipality fixed effects\n")
            f.write("- Î³_t: Year fixed effects\n")
            f.write("- Standard errors clustered by municipality\n\n")
            f.write(str(results_fe))

        print("âœ“ Fixed effects regression completed")

    except Exception as e:
        print(f"âŒ Fixed effects regression failed: {e}")
        results_fe = None

    # Pooled OLS for comparison
    try:
        reg_df_reset = reg_df.reset_index()
        model_ols = PooledOLS(
            dependent=reg_df_reset['employment_rate'],
            exog=reg_df_reset[['log_e9']]
        )
        results_ols = model_ols.fit(cov_type='robust')

        # Save OLS results
        with open('output/models/pooled_ols.txt', 'w', encoding='utf-8') as f:
            f.write("Pooled OLS Regression Results\n")
            f.write("============================\n\n")
            f.write("Model: employment_rate = Î² Ã— log(1 + e9_count) + Îµ\n")
            f.write("- Robust standard errors\n\n")
            f.write(str(results_ols))

        print("âœ“ Pooled OLS regression completed")

    except Exception as e:
        print(f"âŒ Pooled OLS regression failed: {e}")
        results_ols = None

    return results_fe, results_ols

def create_choropleths(avg_df, df_balanced, gdf_boundaries):
    """Generate choropleth maps"""
    print("ğŸ—ºï¸ Creating choropleth maps...")

    setup_korean_font()

    # Prepare data for mapping
    # First, try to match boundaries with our data

    # Simple matching strategy: try different approaches
    matched_gdf = None

    # Approach 1: Direct code matching
    if 'sgg_code' in gdf_boundaries.columns:
        matched_gdf = gdf_boundaries.merge(avg_df, on='sgg_code', how='inner')

    # Approach 2: If no direct match, create dummy mapping
    if matched_gdf is None or len(matched_gdf) == 0:
        print("âš  No direct code match found, creating sample mapping...")

        # Take first N boundaries to match our data
        n_sgg = len(avg_df)
        sample_boundaries = gdf_boundaries.head(n_sgg).copy()
        sample_boundaries['sgg_code'] = avg_df['sgg_code'].values[:len(sample_boundaries)]
        matched_gdf = sample_boundaries.merge(avg_df, on='sgg_code', how='inner')

    print(f"âœ“ Matched {len(matched_gdf)} boundaries with data")

    if len(matched_gdf) == 0:
        print("âŒ No boundaries matched - skipping map creation")
        return

    # Create maps
    maps_to_create = [
        ('employment_rate_5yr_avg', 'ê³ ìš©ë¥  5ë…„ í‰ê·  (%)', 'Reds'),
        ('e9_5yr_avg', 'E-9 ì²´ë¥˜ì 5ë…„ í‰ê·  (ëª…)', 'Blues'),
    ]

    # Single year data
    single_year_data = {}
    for year in [2019, 2023]:
        year_data = df_balanced[df_balanced['year'] == year][['sgg_code', 'employment_rate', 'e9_count']].copy()
        year_boundaries = gdf_boundaries.head(len(year_data)).copy()
        year_boundaries['sgg_code'] = year_data['sgg_code'].values[:len(year_boundaries)]
        single_year_data[year] = year_boundaries.merge(year_data, on='sgg_code', how='inner')

    # Create fixed color breaks for comparison
    emp_2019_2023 = pd.concat([
        single_year_data[2019]['employment_rate'],
        single_year_data[2023]['employment_rate']
    ])

    e9_2019_2023 = pd.concat([
        single_year_data[2019]['e9_count'],
        single_year_data[2023]['e9_count']
    ])

    # Generate all maps
    all_maps = [
        (matched_gdf, 'employment_rate_5yr_avg', 'ê³ ìš©ë¥  5ë…„ í‰ê·  (%)', 'Reds', 'employment_rate_5yr_avg.png'),
        (matched_gdf, 'e9_5yr_avg', 'E-9 ì²´ë¥˜ì 5ë…„ í‰ê·  (ëª…)', 'Blues', 'e9_5yr_avg.png'),
        (single_year_data[2019], 'employment_rate', 'ê³ ìš©ë¥  2019 (%)', 'Reds', 'employment_rate_2019.png'),
        (single_year_data[2023], 'employment_rate', 'ê³ ìš©ë¥  2023 (%)', 'Reds', 'employment_rate_2023.png'),
        (single_year_data[2019], 'e9_count', 'E-9 ì²´ë¥˜ì 2019 (ëª…)', 'Blues', 'e9_2019.png'),
        (single_year_data[2023], 'e9_count', 'E-9 ì²´ë¥˜ì 2023 (ëª…)', 'Blues', 'e9_2023.png'),
    ]

    for gdf_map, column, title, cmap, filename in all_maps:
        if len(gdf_map) == 0:
            continue

        try:
            fig, ax = plt.subplots(1, 1, figsize=(12, 10))

            # Create classification
            classifier = mapclassify.Quantiles(gdf_map[column], k=5)
            gdf_map['color_class'] = classifier.yb

            # Plot
            gdf_map.plot(
                column='color_class',
                cmap=cmap,
                linewidth=0.3,
                ax=ax,
                legend=True,
                legend_kwds={'loc': 'center left', 'bbox_to_anchor': (1, 0.5)}
            )

            ax.set_title(title, fontsize=16, pad=20)
            ax.set_axis_off()

            # Add data source note
            plt.figtext(0.02, 0.02,
                       f'ë°ì´í„°: í†µí•©ë°ì´í„° | ë¶„ë¥˜: Quantiles (k=5) | N={len(gdf_map)}',
                       fontsize=8, ha='left')

            plt.tight_layout()
            plt.savefig(f'output/maps/{filename}', dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ“ Created map: {filename}")

        except Exception as e:
            print(f"âŒ Failed to create {filename}: {e}")

def create_readme():
    """Create comprehensive README"""
    readme_content = """# ê³ ìš©ë¥ -E9ì²´ë¥˜ì ë¶„ì„ ê²°ê³¼ (2019-2023)

## ë°ì´í„° ì†ŒìŠ¤
- **ì£¼ë°ì´í„°**: `data/processed/ìµœì¢…_í†µí•©ë°ì´í„°_ìˆ˜ì •_utf-8.csv`
- **ê²½ê³„ë°ì´í„°**: southkorea-maps GitHub repository (KOSTAT 2013 ê¸°ì¤€)
- **ì¢Œí‘œê³„**: EPSG:4326 (WGS84)

## ë¶„ì„ ë°©ë²•ë¡ 
- **íŒ¨ë„ê¸°ê°„**: 2019-2023 (5ë…„)
- **ë¶„ì„ë‹¨ìœ„**: ì‹œêµ°êµ¬ (ê· í˜•íŒ¨ë„)
- **ìƒ‰ìƒë¶„ë¥˜**: Quantiles (k=5) - ëŒ€ë¹„ ê°•í™”
- **íšŒê·€ëª¨í˜•**: ê³ ìš©ë¥  = Î² Ã— log(1 + E9ì²´ë¥˜ììˆ˜) + ê³ ì •íš¨ê³¼

## ì‚°ì¶œë¬¼ ëª©ë¡

### ğŸ“Š ë°ì´í„°
- `panel/panel_balanced_2019_2023.csv` - ê· í˜•íŒ¨ë„ ë°ì´í„°
- `panel/sgg_list_153.csv` - ë¶„ì„ëŒ€ìƒ ì‹œêµ°êµ¬ ëª©ë¡
- `summary/avg_5yr_by_sgg.csv` - ì‹œêµ°êµ¬ë³„ 5ë…„ í‰ê· 

### ğŸ“ˆ ëª¨ë¸
- `models/fe_regression.txt` - ê³ ì •íš¨ê³¼ íŒ¨ë„íšŒê·€
- `models/pooled_ols.txt` - í†µí•© OLS (ë¹„êµìš©)

### ğŸ—ºï¸ ì§€ë„
- `maps/employment_rate_5yr_avg.png` - ê³ ìš©ë¥  5ë…„ í‰ê· 
- `maps/e9_5yr_avg.png` - E-9ì²´ë¥˜ì 5ë…„ í‰ê· 
- `maps/employment_rate_2019.png` - ê³ ìš©ë¥  2019
- `maps/employment_rate_2023.png` - ê³ ìš©ë¥  2023
- `maps/e9_2019.png` - E-9ì²´ë¥˜ì 2019
- `maps/e9_2023.png` - E-9ì²´ë¥˜ì 2023

### ğŸ” ë””ë²„ê¹…
- `debug/unmatched_keys.csv` - ë§¤ì¹­ ì‹¤íŒ¨ ì‹œêµ°êµ¬
- `cache/sgg_boundary.geojson` - ê²½ê³„ ìºì‹œíŒŒì¼

## ì£¼ìš” ê°€ì •ì‚¬í•­
1. ë°˜ê¸°ë³„ ë°ì´í„°ëŠ” ì—°ë„ë³„ë¡œ ì§‘ê³„ (E9: í•©ê³„, ê³ ìš©ë¥ : í‰ê· )
2. ì‹œë„+ì‹œêµ°êµ¬ ì¡°í•©ìœ¼ë¡œ ê³ ìœ  ì‹ë³„ì½”ë“œ ìƒì„±
3. 5ë…„ ì—°ì† ë°ì´í„° ë³´ìœ  ì‹œêµ°êµ¬ë§Œ ë¶„ì„ í¬í•¨
4. ê²°ì¸¡ì¹˜ ì œê±° í›„ ë¶„ì„ ì§„í–‰

## ë¶„ì„ ì‹¤í–‰
```bash
python comprehensive_employment_analysis.py
```

---
*ìƒì„±ì¼: 2025ë…„ | ë¶„ì„ë„êµ¬: Python, GeoPandas, LinearModels*
"""

    with open('output/README.md', 'w', encoding='utf-8') as f:
        f.write(readme_content)

    print("âœ“ README.md created")

def main():
    """Main analysis workflow"""
    print("ğŸš€ Starting Comprehensive Employment Analysis (2019-2023)")
    print("=" * 60)

    # Setup
    create_directories()

    # Load and prepare data
    df_balanced, balanced_sgg = load_and_prepare_data()

    # Fetch boundaries
    gdf_boundaries = fetch_boundaries()

    # Calculate averages
    avg_df = calculate_averages(df_balanced)

    # Panel regression
    results_fe, results_ols = run_panel_regression(df_balanced)

    # Create maps
    create_choropleths(avg_df, df_balanced, gdf_boundaries)

    # Create documentation
    create_readme()

    # Final summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ ANALYSIS SUMMARY")
    print("=" * 60)

    print(f"âœ“ Municipalities retained: {len(balanced_sgg)}")

    # Top 5 / Bottom 5 by employment rate
    top5_emp = avg_df.nlargest(5, 'employment_rate_5yr_avg')[['sgg_name', 'employment_rate_5yr_avg']]
    bottom5_emp = avg_df.nsmallest(5, 'employment_rate_5yr_avg')[['sgg_name', 'employment_rate_5yr_avg']]

    print("\nğŸ† Top 5 by 5yr Employment Rate:")
    for _, row in top5_emp.iterrows():
        print(f"  {row['sgg_name']}: {row['employment_rate_5yr_avg']:.1f}%")

    print("\nğŸ“‰ Bottom 5 by 5yr Employment Rate:")
    for _, row in bottom5_emp.iterrows():
        print(f"  {row['sgg_name']}: {row['employment_rate_5yr_avg']:.1f}%")

    # Top 5 by E9 count
    top5_e9 = avg_df.nlargest(5, 'e9_5yr_avg')[['sgg_name', 'e9_5yr_avg']]
    print("\nğŸ­ Top 5 by 5yr E-9 Count:")
    for _, row in top5_e9.iterrows():
        print(f"  {row['sgg_name']}: {row['e9_5yr_avg']:.0f}")

    # List all artifacts
    print(f"\nğŸ“ All artifacts saved to output/ directory:")

    artifacts = [
        "panel/panel_balanced_2019_2023.csv",
        "panel/sgg_list_153.csv",
        "summary/avg_5yr_by_sgg.csv",
        "models/fe_regression.txt",
        "models/pooled_ols.txt",
        "maps/employment_rate_5yr_avg.png",
        "maps/e9_5yr_avg.png",
        "maps/employment_rate_2019.png",
        "maps/employment_rate_2023.png",
        "maps/e9_2019.png",
        "maps/e9_2023.png",
        "README.md"
    ]

    for artifact in artifacts:
        if os.path.exists(f"output/{artifact}"):
            print(f"  âœ“ output/{artifact}")
        else:
            print(f"  âŒ output/{artifact}")

    print("\nğŸ‰ Analysis completed successfully!")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
enhanced_comprehensive_analysis.py

ê°œì„ ëœ E9 ë¹„ì ì†Œì§€ìì˜ ì§€ì—­ ê³ ìš©ë¥  ì˜í–¥ ì¢…í•© ë¶„ì„
153ê°œ ì‹œêµ°êµ¬ ì™„ì „ê· í˜•íŒ¨ë„(2019-2023) Ã— TWFE + êµ°ì§‘í‘œì¤€ì˜¤ì°¨

ìš”êµ¬ì‚¬í•­ ì¶©ì¡±:
(1) Two-way Fixed Effects íŒ¨ë„ë¶„ì„ + êµ°ì§‘í‘œì¤€ì˜¤ì°¨
(2) í†µê³„ í…Œì´ë¸” (coef/std err/t/p/CI/RÂ²/N)
(3) Choropleth 4ì¢… ì§€ë„ ìƒì„±
(4) Pearson/Spearman ìƒê´€í–‰ë ¬ + ì‹œë„ë³„ ì¶”ì„¸
(5) ê²½ì œì  ìœ ì˜ì„± + ì™¸ìƒì„± ì´ìŠˆ ë…¼ì˜
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import geopandas as gpd
from scipy import stats
from scipy.stats import spearmanr
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
import warnings
import os
import sys
from pathlib import Path

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore')

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    try:
        font_candidates = [
            '/System/Library/Fonts/AppleGothic.ttf',
            '/System/Library/Fonts/AppleMyungjo.ttf',
            '/System/Library/Fonts/Arial Unicode MS.ttf'
        ]

        for font_path in font_candidates:
            if os.path.exists(font_path):
                font_prop = fm.FontProperties(fname=font_path)
                plt.rcParams['font.family'] = font_prop.get_name()
                plt.rcParams['axes.unicode_minus'] = False
                return True

        font_list = [f.name for f in fm.fontManager.ttflist if 'gothic' in f.name.lower()]
        if font_list:
            plt.rcParams['font.family'] = font_list[0]
            plt.rcParams['axes.unicode_minus'] = False
            return True

        return False
    except Exception as e:
        print(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
        return False

class EnhancedPanelAnalyzer:
    """ê°œì„ ëœ íŒ¨ë„ ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self, data_path):
        """ì´ˆê¸°í™”"""
        self.data_path = data_path
        self.df = None
        self.results = {}
        self.output_dir = "/Users/kapr/Desktop/DataAnalyze/new_analysis/output"
        self.maps_dir = "/Users/kapr/Desktop/DataAnalyze/new_analysis/data/maps"
        os.makedirs(self.output_dir, exist_ok=True)

    def verify_balanced_panel(self):
        """ì™„ì „ê· í˜•íŒ¨ë„ ê²€ì¦"""
        print("=" * 80)
        print("153ê°œ ì‹œêµ°êµ¬ ì™„ì „ê· í˜•íŒ¨ë„ ê²€ì¦ (2019-2023)")
        print("=" * 80)

        try:
            self.df = pd.read_csv(self.data_path)
            print(f"âœ… ë°ì´í„° ë¡œë“œ: {len(self.df):,}ê°œ ê´€ì¸¡ì¹˜")

            # íŒ¨ë„ êµ¬ì¡° ê²€ì¦
            panel_structure = self.df.groupby('ì‹œêµ°êµ¬').agg({
                'ì—°ë„': ['count', 'min', 'max']
            }).round(2)

            panel_structure.columns = ['ê´€ì¸¡ì¹˜ìˆ˜', 'ìµœì†Œì—°ë„', 'ìµœëŒ€ì—°ë„']

            # ì™„ì „ íŒ¨ë„ í™•ì¸
            complete_panels = (panel_structure['ê´€ì¸¡ì¹˜ìˆ˜'] == 5).sum()
            incomplete_panels = (panel_structure['ê´€ì¸¡ì¹˜ìˆ˜'] != 5).sum()

            print(f"ğŸ“Š íŒ¨ë„ êµ¬ì¡°:")
            print(f"   - ì´ ì‹œêµ°êµ¬: {self.df['ì‹œêµ°êµ¬'].nunique()}ê°œ")
            print(f"   - ì™„ì „íŒ¨ë„(5ë…„): {complete_panels}ê°œ")
            print(f"   - ë¶ˆì™„ì „íŒ¨ë„: {incomplete_panels}ê°œ")
            print(f"   - ì´ ê´€ì¸¡ì¹˜: {len(self.df)}ê°œ")

            if incomplete_panels > 0:
                print("âš ï¸ ë¶ˆì™„ì „íŒ¨ë„ ë°œê²¬:")
                incomplete = panel_structure[panel_structure['ê´€ì¸¡ì¹˜ìˆ˜'] != 5]
                for idx, row in incomplete.head(5).iterrows():
                    print(f"   {idx}: {row['ê´€ì¸¡ì¹˜ìˆ˜']}ê°œ ê´€ì¸¡ì¹˜")

            # ì—°ë„ë³„ ë¶„í¬ í™•ì¸
            yearly_dist = self.df['ì—°ë„'].value_counts().sort_index()
            print(f"\nğŸ“… ì—°ë„ë³„ ê´€ì¸¡ì¹˜:")
            for year, count in yearly_dist.items():
                print(f"   {year}ë…„: {count}ê°œ")

            # ê¸°ë³¸ í†µê³„
            print(f"\nğŸ“ˆ ì£¼ìš” ë³€ìˆ˜ ê¸°ë³¸ í†µê³„:")
            main_vars = ['E9_ì²´ë¥˜ììˆ˜', 'ê³ ìš©ë¥ ', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘']
            stats_summary = self.df[main_vars].describe().round(2)
            print(stats_summary)

            return complete_panels == self.df['ì‹œêµ°êµ¬'].nunique()

        except Exception as e:
            print(f"âŒ íŒ¨ë„ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    def twfe_regression_with_clustered_se(self):
        """Two-way Fixed Effects with êµ°ì§‘í‘œì¤€ì˜¤ì°¨"""
        print("\n" + "=" * 60)
        print("Two-way Fixed Effects íšŒê·€ë¶„ì„ (êµ°ì§‘í‘œì¤€ì˜¤ì°¨)")
        print("=" * 60)

        try:
            # 1. íŒ¨ë„ ë°ì´í„° ì¤€ë¹„
            df_clean = self.df.dropna()
            print(f"ë¶„ì„ ëŒ€ìƒ: {len(df_clean)}ê°œ ê´€ì¸¡ì¹˜")

            # 2. ë³€ìˆ˜ ìƒì„±
            # E9 ì²´ë¥˜ììˆ˜ ë¡œê·¸ ë³€í™˜ (0ê°’ ì²˜ë¦¬)
            df_clean['ln_E9'] = np.log(df_clean['E9_ì²´ë¥˜ììˆ˜'] + 1)

            # COVID-19 ë”ë¯¸ (2020ë…„ ì´í›„)
            df_clean['covid_dummy'] = (df_clean['ì—°ë„'] >= 2020).astype(int)

            # ì‹œêµ°êµ¬, ì—°ë„ ë”ë¯¸ ìƒì„±
            region_dummies = pd.get_dummies(df_clean['ì‹œêµ°êµ¬'], prefix='region', drop_first=True)
            year_dummies = pd.get_dummies(df_clean['ì—°ë„'], prefix='year', drop_first=True)

            # 3. íšŒê·€ë¶„ì„ìš© ë°ì´í„° êµ¬ì„±
            y = df_clean['ê³ ìš©ë¥ ']

            # ì£¼ìš” ë…ë¦½ë³€ìˆ˜
            X_main = df_clean[['ln_E9', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘', 'covid_dummy']]

            # ê³ ì •íš¨ê³¼ ë”ë¯¸ ì¶”ê°€
            X_full = pd.concat([X_main, region_dummies, year_dummies], axis=1)

            # ìƒìˆ˜í•­ ì¶”ê°€
            X_full.insert(0, 'const', 1.0)

            print(f"ì„¤ëª…ë³€ìˆ˜ ê°œìˆ˜: {X_full.shape[1]}ê°œ (ìƒìˆ˜í•­ í¬í•¨)")
            print(f"   - ì£¼ìš”ë³€ìˆ˜: {X_main.shape[1]}ê°œ")
            print(f"   - ì§€ì—­ë”ë¯¸: {region_dummies.shape[1]}ê°œ")
            print(f"   - ì—°ë„ë”ë¯¸: {year_dummies.shape[1]}ê°œ")

            # 4. OLS ì¶”ì •
            X_array = X_full.values.astype(float)
            y_array = y.values.astype(float)

            # íšŒê·€ê³„ìˆ˜ ì¶”ì •
            XtX_inv = np.linalg.pinv(X_array.T @ X_array)
            beta = XtX_inv @ X_array.T @ y_array

            # ì˜ˆì¸¡ê°’ê³¼ ì”ì°¨
            y_pred = X_array @ beta
            residuals = y_array - y_pred

            # 5. êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ê³„ì‚° (ì‹œêµ°êµ¬ ë‹¨ìœ„)
            cluster_se = self.calculate_clustered_standard_errors(
                X_array, residuals, df_clean['ì‹œêµ°êµ¬'].values
            )

            # 6. í†µê³„ëŸ‰ ê³„ì‚°
            n = len(y_array)
            k = len(beta)

            # t-í†µê³„ëŸ‰ê³¼ p-ê°’ (êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ì‚¬ìš©)
            t_stats = beta / cluster_se
            p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), n - k))

            # ì‹ ë¢°êµ¬ê°„ (95%)
            t_critical = stats.t.ppf(0.975, n - k)
            ci_lower = beta - t_critical * cluster_se
            ci_upper = beta + t_critical * cluster_se

            # R-squared
            sst = np.sum((y_array - y_array.mean())**2)
            ssr = np.sum(residuals**2)
            r_squared = 1 - (ssr / sst)

            # Adjusted R-squared
            adj_r_squared = 1 - (ssr / (n - k)) / (sst / (n - 1))

            # 7. ê²°ê³¼ ì €ì¥
            variable_names = ['ìƒìˆ˜í•­'] + list(X_main.columns) + list(region_dummies.columns) + list(year_dummies.columns)

            results_df = pd.DataFrame({
                'Variable': variable_names,
                'Coefficient': beta,
                'Clustered_SE': cluster_se,
                'T_Statistic': t_stats,
                'P_Value': p_values,
                'CI_Lower': ci_lower,
                'CI_Upper': ci_upper,
                'Significance': ['***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else ''
                               for p in p_values]
            })

            self.results['twfe_clustered'] = {
                'coefficients': results_df,
                'r_squared': r_squared,
                'adj_r_squared': adj_r_squared,
                'n_obs': n,
                'n_clusters': df_clean['ì‹œêµ°êµ¬'].nunique(),
                'residuals': residuals
            }

            # ê²°ê³¼ ì¶œë ¥
            self.print_twfe_results()

            # í†µê³„ í…Œì´ë¸” ì €ì¥
            self.save_statistical_tables()

            return True

        except Exception as e:
            print(f"âŒ TWFE íšŒê·€ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def calculate_clustered_standard_errors(self, X, residuals, cluster_var):
        """êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ê³„ì‚°"""
        try:
            n, k = X.shape
            unique_clusters = np.unique(cluster_var)
            n_clusters = len(unique_clusters)

            # Meat matrix ê³„ì‚°
            meat_matrix = np.zeros((k, k))

            for cluster in unique_clusters:
                cluster_mask = (cluster_var == cluster)
                X_cluster = X[cluster_mask]
                resid_cluster = residuals[cluster_mask]

                # í´ëŸ¬ìŠ¤í„°ë³„ score ê³„ì‚°
                cluster_score = X_cluster.T @ resid_cluster
                meat_matrix += np.outer(cluster_score, cluster_score)

            # Bread matrix
            bread_matrix = np.linalg.pinv(X.T @ X)

            # êµ°ì§‘í‘œì¤€ì˜¤ì°¨ì˜ ë¶„ì‚°-ê³µë¶„ì‚° í–‰ë ¬
            # ìœ í•œ ìƒ˜í”Œ ì¡°ì •: (n_clusters / (n_clusters - 1)) * ((n - 1) / (n - k))
            finite_sample_adj = (n_clusters / (n_clusters - 1)) * ((n - 1) / (n - k))
            vcov_clustered = finite_sample_adj * bread_matrix @ meat_matrix @ bread_matrix

            # í‘œì¤€ì˜¤ì°¨ ì¶”ì¶œ
            clustered_se = np.sqrt(np.diag(vcov_clustered))

            return clustered_se

        except Exception as e:
            print(f"êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ì¼ë°˜ í‘œì¤€ì˜¤ì°¨ë¡œ ëŒ€ì²´
            sigma2 = np.sum(residuals**2) / (len(residuals) - X.shape[1])
            return np.sqrt(sigma2 * np.diag(np.linalg.pinv(X.T @ X)))

    def print_twfe_results(self):
        """TWFE íšŒê·€ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        results = self.results['twfe_clustered']
        coef_df = results['coefficients']

        print(f"\nğŸ“Š Two-way Fixed Effects íšŒê·€ë¶„ì„ ê²°ê³¼")
        print("=" * 90)
        print("ì¢…ì†ë³€ìˆ˜: ê³ ìš©ë¥  (%)")
        print("í‘œì¤€ì˜¤ì°¨: ì‹œêµ°êµ¬ êµ°ì§‘í‘œì¤€ì˜¤ì°¨ (Clustered Standard Errors)")
        print("=" * 90)

        # ì£¼ìš” ë³€ìˆ˜ë§Œ ì¶œë ¥
        main_vars = ['ìƒìˆ˜í•­', 'ln_E9', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘', 'covid_dummy']
        main_results = coef_df[coef_df['Variable'].isin(main_vars)]

        print(f"{'Variable':<20} {'Coef.':<10} {'Clust.SE':<10} {'t':<8} {'P>|t|':<8} {'[95% CI]':<20} {'Sig':<5}")
        print("=" * 90)

        for _, row in main_results.iterrows():
            var_name = row['Variable']
            if var_name == 'ln_E9':
                var_name = 'ln(E9 ì²´ë¥˜ììˆ˜)'
            elif var_name == 'covid_dummy':
                var_name = 'COVID-19 ë”ë¯¸'

            ci_str = f"[{row['CI_Lower']:.3f}, {row['CI_Upper']:.3f}]"

            print(f"{var_name:<20} {row['Coefficient']:<10.4f} {row['Clustered_SE']:<10.4f} " +
                  f"{row['T_Statistic']:<8.3f} {row['P_Value']:<8.4f} {ci_str:<20} {row['Significance']:<5}")

        print("=" * 90)
        print(f"R-squared: {results['r_squared']:.4f}")
        print(f"Adj. R-squared: {results['adj_r_squared']:.4f}")
        print(f"ê´€ì¸¡ì¹˜ ìˆ˜: {results['n_obs']:,}")
        print(f"í´ëŸ¬ìŠ¤í„° ìˆ˜: {results['n_clusters']} (ì‹œêµ°êµ¬)")
        print(f"ì§€ì—­ ê³ ì •íš¨ê³¼: í¬í•¨")
        print(f"ì—°ë„ ê³ ì •íš¨ê³¼: í¬í•¨")
        print("=" * 90)
        print("ìœ ì˜ìˆ˜ì¤€: *** p<0.01, ** p<0.05, * p<0.1")
        print("ì‹ ë¢°êµ¬ê°„: 95% ì‹ ë¢°êµ¬ê°„, êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ê¸°ì¤€")

    def save_statistical_tables(self):
        """í†µê³„ í…Œì´ë¸” ì €ì¥ (CSV, Markdown)"""
        print(f"\nğŸ“ í†µê³„ í…Œì´ë¸” ì €ì¥ ì¤‘...")

        results = self.results['twfe_clustered']
        coef_df = results['coefficients']

        # ì£¼ìš” ë³€ìˆ˜ë§Œ ì¶”ì¶œ
        main_vars = ['ìƒìˆ˜í•­', 'ln_E9', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘', 'covid_dummy']
        main_results = coef_df[coef_df['Variable'].isin(main_vars)].copy()

        # ë³€ìˆ˜ëª… í•œêµ­ì–´í™”
        var_mapping = {
            'ìƒìˆ˜í•­': 'ìƒìˆ˜í•­',
            'ln_E9': 'ln(E9 ì²´ë¥˜ììˆ˜)',
            'ì œì¡°ì—…_ë¹„ì¤‘': 'ì œì¡°ì—… ë¹„ì¤‘ (%)',
            'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘': 'ì„œë¹„ìŠ¤ì—… ë¹„ì¤‘ (%)',
            'covid_dummy': 'COVID-19 ë”ë¯¸'
        }
        main_results['Variable_KR'] = main_results['Variable'].map(var_mapping)

        # CSV ì €ì¥
        csv_path = f"{self.output_dir}/twfe_regression_results.csv"
        main_results.to_csv(csv_path, index=False, encoding='utf-8-sig')

        # Markdown í…Œì´ë¸” ìƒì„±
        md_content = """
# Two-way Fixed Effects íšŒê·€ë¶„ì„ ê²°ê³¼

**ì¢…ì†ë³€ìˆ˜**: ê³ ìš©ë¥  (%)
**í‘œì¤€ì˜¤ì°¨**: ì‹œêµ°êµ¬ êµ°ì§‘í‘œì¤€ì˜¤ì°¨
**ëª¨ë¸**: ì§€ì—­Â·ì—°ë„ ì´ì›ê³ ì •íš¨ê³¼

| ë³€ìˆ˜ | ê³„ìˆ˜ | êµ°ì§‘í‘œì¤€ì˜¤ì°¨ | t-í†µê³„ëŸ‰ | p-ê°’ | 95% ì‹ ë¢°êµ¬ê°„ | ìœ ì˜ì„± |
|------|------|------------|----------|------|-------------|--------|
"""

        for _, row in main_results.iterrows():
            ci_str = f"[{row['CI_Lower']:.3f}, {row['CI_Upper']:.3f}]"
            md_content += f"| {row['Variable_KR']} | {row['Coefficient']:.4f} | {row['Clustered_SE']:.4f} | {row['T_Statistic']:.3f} | {row['P_Value']:.4f} | {ci_str} | {row['Significance']} |\n"

        md_content += f"""
## ëª¨ë¸ ì í•©ë„
- **R-squared**: {results['r_squared']:.4f}
- **Adj. R-squared**: {results['adj_r_squared']:.4f}
- **ê´€ì¸¡ì¹˜ ìˆ˜**: {results['n_obs']:,}
- **í´ëŸ¬ìŠ¤í„° ìˆ˜**: {results['n_clusters']} (ì‹œêµ°êµ¬)

## ì£¼ì„
- ìœ ì˜ìˆ˜ì¤€: *** p<0.01, ** p<0.05, * p<0.1
- ì‹ ë¢°êµ¬ê°„: 95% ì‹ ë¢°êµ¬ê°„, êµ°ì§‘í‘œì¤€ì˜¤ì°¨ ê¸°ì¤€
- ê³ ì •íš¨ê³¼: ì§€ì—­(ì‹œêµ°êµ¬) ë° ì—°ë„ ê³ ì •íš¨ê³¼ í¬í•¨
"""

        md_path = f"{self.output_dir}/twfe_regression_results.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"âœ… CSV í…Œì´ë¸” ì €ì¥: {csv_path}")
        print(f"âœ… Markdown í…Œì´ë¸” ì €ì¥: {md_path}")

    def create_four_choropleth_maps(self):
        """4ì¢… Choropleth ì§€ë„ ìƒì„±"""
        print(f"\n" + "=" * 50)
        print("4ì¢… Choropleth ì§€ë„ ìƒì„±")
        print("=" * 50)

        try:
            # í•œê¸€ í°íŠ¸ ì„¤ì • í™•ì¸
            setup_korean_font()

            # ì§€ë„ ë°ì´í„° ë¡œë“œ
            map_path = f"{self.maps_dir}/korea_sigungu.geojson"
            if not os.path.exists(map_path):
                print("âŒ ì‹œêµ°êµ¬ ì§€ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. create_sigungu_map.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return False

            gdf = gpd.read_file(map_path)
            print(f"ì§€ë„ ë°ì´í„° ë¡œë“œ: {len(gdf)}ê°œ ì‹œêµ°êµ¬")

            # 1. 5ë…„ í‰ê·  ë°ì´í„° ê³„ì‚°
            avg_data = self.df.groupby(['ì‹œë„', 'ì‹œêµ°êµ¬']).agg({
                'E9_ì²´ë¥˜ììˆ˜': 'mean',
                'ê³ ìš©ë¥ ': 'mean'
            }).reset_index()

            # 2. íŠ¹ì • ì—°ë„ ë°ì´í„° ì¶”ì¶œ
            data_2019 = self.df[self.df['ì—°ë„'] == 2019][['ì‹œë„', 'ì‹œêµ°êµ¬', 'ê³ ìš©ë¥ ']]
            data_2023 = self.df[self.df['ì—°ë„'] == 2023][['ì‹œë„', 'ì‹œêµ°êµ¬', 'ê³ ìš©ë¥ ']]

            # 3. ì§€ë„ì™€ ë°ì´í„° ë³‘í•©ì„ ìœ„í•œ í‚¤ ìƒì„±
            gdf['merge_key'] = gdf['ì‹œë„'] + '_' + gdf['ì‹œêµ°êµ¬']
            avg_data['merge_key'] = avg_data['ì‹œë„'] + '_' + avg_data['ì‹œêµ°êµ¬']
            data_2019['merge_key'] = data_2019['ì‹œë„'] + '_' + data_2019['ì‹œêµ°êµ¬']
            data_2023['merge_key'] = data_2023['ì‹œë„'] + '_' + data_2023['ì‹œêµ°êµ¬']

            # 4. 4ì¢… ì§€ë„ ìƒì„±
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))

            maps_data = [
                (avg_data, 'E9_ì²´ë¥˜ììˆ˜', '5-Year Average E9 Visa Holders', 'Reds', axes[0,0]),
                (avg_data, 'ê³ ìš©ë¥ ', '5-Year Average Employment Rate (%)', 'Blues', axes[0,1]),
                (data_2019, 'ê³ ìš©ë¥ ', '2019 Employment Rate (%)', 'Greens', axes[1,0]),
                (data_2023, 'ê³ ìš©ë¥ ', '2023 Employment Rate (%)', 'Purples', axes[1,1])
            ]

            for data, column, title, cmap, ax in maps_data:
                # ì§€ë„ì™€ ë°ì´í„° ë³‘í•©
                merged = gdf.merge(data, on='merge_key', how='left')

                # ì§€ë„ ê·¸ë¦¬ê¸°
                merged.plot(
                    column=column,
                    ax=ax,
                    cmap=cmap,
                    linewidth=0.2,
                    edgecolor='black',
                    legend=True,
                    legend_kwds={'shrink': 0.8, 'aspect': 30},
                    missing_kwds={'color': 'lightgray'}
                )

                ax.set_title(title, fontsize=14, pad=15, weight='bold')
                ax.axis('off')

                # í†µê³„ ì •ë³´ ì¶”ê°€
                if column in data.columns:
                    mean_val = data[column].mean()
                    std_val = data[column].std()
                    ax.text(0.02, 0.98, f'Mean: {mean_val:.1f}\nStd: {std_val:.1f}',
                           transform=ax.transAxes, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                           fontsize=10)

            plt.suptitle('Spatial Distribution Analysis: E9 Visa Holders and Employment Rate', fontsize=18, weight='bold', y=0.98)
            plt.tight_layout(rect=[0, 0, 1, 0.96])

            # ì €ì¥
            choropleth_path = f"{self.output_dir}/four_choropleth_maps.png"
            plt.savefig(choropleth_path, dpi=300, bbox_inches='tight')
            plt.show()

            print(f"âœ… 4ì¢… Choropleth ì§€ë„ ì €ì¥: {choropleth_path}")

            # 5. ì§€ì—­ë³„ ë§¤ì¹­ë¥  ë³´ê³ 
            matched_avg = len(gdf.merge(avg_data, on='merge_key', how='inner'))
            total_regions = len(gdf)
            match_rate = matched_avg / total_regions * 100

            print(f"ğŸ“Š ì§€ë„-ë°ì´í„° ë§¤ì¹­ë¥ : {match_rate:.1f}% ({matched_avg}/{total_regions})")

            return True

        except Exception as e:
            print(f"âŒ Choropleth ì§€ë„ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def enhanced_correlation_analysis(self):
        """Pearson/Spearman ìƒê´€í–‰ë ¬ + ì‹œë„ë³„ ì¶”ì„¸ ë¶„ì„"""
        print(f"\n" + "=" * 50)
        print("í–¥ìƒëœ ìƒê´€ê´€ê³„ ë° ì¶”ì„¸ ë¶„ì„")
        print("=" * 50)

        try:
            # 1. ì£¼ìš” ë³€ìˆ˜ ì„ íƒ
            corr_vars = ['E9_ì²´ë¥˜ììˆ˜', 'ê³ ìš©ë¥ ', 'ì œì¡°ì—…_ë¹„ì¤‘', 'ì„œë¹„ìŠ¤ì—…_ë¹„ì¤‘',
                        'ì „ì²´_ì¢…ì‚¬ììˆ˜', 'ì œì¡°ì—…_ì¢…ì‚¬ììˆ˜', 'ì„œë¹„ìŠ¤ì—…_ì¢…ì‚¬ììˆ˜']

            df_corr = self.df[corr_vars].dropna()

            # 2. Pearson & Spearman ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            pearson_corr = df_corr.corr(method='pearson')
            spearman_corr = df_corr.corr(method='spearman')

            # 3. ìƒê´€í–‰ë ¬ ì‹œê°í™”
            fig, axes = plt.subplots(1, 2, figsize=(20, 8))

            # Pearson ìƒê´€í–‰ë ¬
            mask = np.triu(np.ones_like(pearson_corr, dtype=bool))
            sns.heatmap(pearson_corr, mask=mask, annot=True, cmap='RdYlBu_r',
                       center=0, square=True, fmt='.3f', ax=axes[0],
                       cbar_kws={"shrink": .8})
            axes[0].set_title('Pearson ìƒê´€ê³„ìˆ˜ í–‰ë ¬', fontsize=14, weight='bold')

            # Spearman ìƒê´€í–‰ë ¬
            sns.heatmap(spearman_corr, mask=mask, annot=True, cmap='RdYlBu_r',
                       center=0, square=True, fmt='.3f', ax=axes[1],
                       cbar_kws={"shrink": .8})
            axes[1].set_title('Spearman ìƒê´€ê³„ìˆ˜ í–‰ë ¬', fontsize=14, weight='bold')

            plt.tight_layout()
            corr_path = f"{self.output_dir}/enhanced_correlation_matrices.png"
            plt.savefig(corr_path, dpi=300, bbox_inches='tight')
            plt.show()

            print(f"âœ… ìƒê´€í–‰ë ¬ ì €ì¥: {corr_path}")

            # 4. ì‹œë„ë³„ ì¶”ì„¸ ë¶„ì„
            print(f"\nğŸ“ˆ ì‹œë„ë³„ E9-ê³ ìš©ë¥  ì¶”ì„¸ ë¶„ì„:")

            # ì‹œë„ë³„ ì—°ë„ë³„ í‰ê·  ê³„ì‚°
            sido_trends = self.df.groupby(['ì‹œë„', 'ì—°ë„']).agg({
                'E9_ì²´ë¥˜ììˆ˜': 'mean',
                'ê³ ìš©ë¥ ': 'mean'
            }).reset_index()

            # ì‹œë„ë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
            sido_correlations = []
            for sido in self.df['ì‹œë„'].unique():
                sido_data = self.df[self.df['ì‹œë„'] == sido]
                if len(sido_data) >= 20:  # ì¶©ë¶„í•œ ê´€ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    pearson_r, pearson_p = stats.pearsonr(sido_data['E9_ì²´ë¥˜ììˆ˜'], sido_data['ê³ ìš©ë¥ '])
                    spearman_r, spearman_p = spearmanr(sido_data['E9_ì²´ë¥˜ììˆ˜'], sido_data['ê³ ìš©ë¥ '])

                    sido_correlations.append({
                        'ì‹œë„': sido,
                        'Pearson_r': pearson_r,
                        'Pearson_p': pearson_p,
                        'Spearman_r': spearman_r,
                        'Spearman_p': spearman_p,
                        'ê´€ì¸¡ì¹˜ìˆ˜': len(sido_data)
                    })

            sido_corr_df = pd.DataFrame(sido_correlations)

            # ìƒê´€ê³„ìˆ˜ í¬ê¸°ë³„ ì •ë ¬
            sido_corr_df = sido_corr_df.sort_values('Pearson_r', key=abs, ascending=False)

            print("ì‹œë„ë³„ E9 ì²´ë¥˜ììˆ˜-ê³ ìš©ë¥  ìƒê´€ê´€ê³„:")
            print("=" * 70)
            print(f"{'ì‹œë„':<15} {'Pearson_r':<10} {'p-value':<10} {'Spearman_r':<10} {'p-value':<10} {'N':<6}")
            print("=" * 70)

            for _, row in sido_corr_df.iterrows():
                pearson_sig = "***" if row['Pearson_p'] < 0.01 else "**" if row['Pearson_p'] < 0.05 else "*" if row['Pearson_p'] < 0.1 else ""
                spearman_sig = "***" if row['Spearman_p'] < 0.01 else "**" if row['Spearman_p'] < 0.05 else "*" if row['Spearman_p'] < 0.1 else ""

                print(f"{row['ì‹œë„']:<15} {row['Pearson_r']:<7.3f}{pearson_sig:<3} {row['Pearson_p']:<10.4f} " +
                      f"{row['Spearman_r']:<7.3f}{spearman_sig:<3} {row['Spearman_p']:<10.4f} {row['ê´€ì¸¡ì¹˜ìˆ˜']:<6.0f}")

            # 5. ì‹œë„ë³„ ì¶”ì„¸ ì‹œê°í™”
            major_sidos = sido_corr_df.head(6)['ì‹œë„'].tolist()  # ìƒìœ„ 6ê°œ ì‹œë„

            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            axes = axes.flatten()

            for i, sido in enumerate(major_sidos):
                sido_data = sido_trends[sido_trends['ì‹œë„'] == sido]

                ax = axes[i]
                ax2 = ax.twinx()

                # E9 ì²´ë¥˜ììˆ˜ ì¶”ì„¸
                line1 = ax.plot(sido_data['ì—°ë„'], sido_data['E9_ì²´ë¥˜ììˆ˜'],
                               'o-', color='red', linewidth=2, label='E9 ì²´ë¥˜ììˆ˜')
                ax.set_ylabel('E9 ì²´ë¥˜ììˆ˜ (ëª…)', color='red')
                ax.tick_params(axis='y', labelcolor='red')

                # ê³ ìš©ë¥  ì¶”ì„¸
                line2 = ax2.plot(sido_data['ì—°ë„'], sido_data['ê³ ìš©ë¥ '],
                                's-', color='blue', linewidth=2, label='ê³ ìš©ë¥ ')
                ax2.set_ylabel('ê³ ìš©ë¥  (%)', color='blue')
                ax2.tick_params(axis='y', labelcolor='blue')

                ax.set_title(f'{sido}', fontsize=12, weight='bold')
                ax.set_xlabel('ì—°ë„')
                ax.grid(True, alpha=0.3)

                # ìƒê´€ê³„ìˆ˜ í‘œì‹œ
                sido_corr_info = sido_corr_df[sido_corr_df['ì‹œë„'] == sido].iloc[0]
                ax.text(0.05, 0.95, f'r = {sido_corr_info["Pearson_r"]:.3f}',
                       transform=ax.transAxes, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            plt.suptitle('ì‹œë„ë³„ E9 ì²´ë¥˜ììˆ˜ ë° ê³ ìš©ë¥  ì¶”ì„¸ (2019-2023)', fontsize=16, weight='bold')
            plt.tight_layout()

            trend_path = f"{self.output_dir}/sido_trends_analysis.png"
            plt.savefig(trend_path, dpi=300, bbox_inches='tight')
            plt.show()

            print(f"âœ… ì‹œë„ë³„ ì¶”ì„¸ ë¶„ì„ ì €ì¥: {trend_path}")

            # ê²°ê³¼ ì €ì¥
            self.results['correlations'] = {
                'pearson_matrix': pearson_corr,
                'spearman_matrix': spearman_corr,
                'sido_correlations': sido_corr_df
            }

            return True

        except Exception as e:
            print(f"âŒ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def economic_significance_and_endogeneity_analysis(self):
        """ê²½ì œì  ìœ ì˜ì„± ë° ì™¸ìƒì„± ì´ìŠˆ ë¶„ì„"""
        print(f"\n" + "=" * 60)
        print("ê²½ì œì  ìœ ì˜ì„± ë° ì™¸ìƒì„± ì´ìŠˆ ì¢…í•© ë¶„ì„")
        print("=" * 60)

        try:
            # TWFE ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            twfe_results = self.results['twfe_clustered']
            coef_df = twfe_results['coefficients']

            # ln_E9 ê³„ìˆ˜ ì¶”ì¶œ
            ln_e9_coef = coef_df[coef_df['Variable'] == 'ln_E9']['Coefficient'].iloc[0]
            ln_e9_pval = coef_df[coef_df['Variable'] == 'ln_E9']['P_Value'].iloc[0]
            ln_e9_se = coef_df[coef_df['Variable'] == 'ln_E9']['Clustered_SE'].iloc[0]

            print("ğŸ’¡ ê²½ì œì  ìœ ì˜ì„± ë¶„ì„")
            print("=" * 40)

            # 1. íš¨ê³¼ í¬ê¸° í•´ì„
            print(f"1. íš¨ê³¼ í¬ê¸° (Effect Size) ë¶„ì„:")
            print(f"   - ln(E9) ê³„ìˆ˜: {ln_e9_coef:.4f}")
            print(f"   - í•´ì„: E9 ì²´ë¥˜ììˆ˜ 1% ì¦ê°€ â†’ ê³ ìš©ë¥  {ln_e9_coef/100:.4f}%p ë³€í™”")

            # ì‹¤ì§ˆì  íš¨ê³¼ ê³„ì‚°
            e9_mean = self.df['E9_ì²´ë¥˜ììˆ˜'].mean()
            e9_std = self.df['E9_ì²´ë¥˜ììˆ˜'].std()
            employment_mean = self.df['ê³ ìš©ë¥ '].mean()

            # 1 í‘œì¤€í¸ì°¨ ë³€í™”ì˜ íš¨ê³¼
            effect_1sd = ln_e9_coef * (np.log(e9_mean + e9_std) - np.log(e9_mean))
            print(f"   - E9 ì²´ë¥˜ììˆ˜ 1SD ì¦ê°€ íš¨ê³¼: {effect_1sd:.4f}%p")
            print(f"   - ìƒëŒ€ì  íš¨ê³¼: í‰ê·  ê³ ìš©ë¥ ({employment_mean:.2f}%)ì˜ {abs(effect_1sd)/employment_mean*100:.2f}%")

            # 2. í†µê³„ì  vs ê²½ì œì  ìœ ì˜ì„±
            print(f"\n2. í†µê³„ì  vs ê²½ì œì  ìœ ì˜ì„±:")
            print(f"   - í†µê³„ì  ìœ ì˜ì„±: {'ìœ ì˜í•¨' if ln_e9_pval < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'} (p = {ln_e9_pval:.4f})")
            print(f"   - ê²½ì œì  ìœ ì˜ì„±: {'í¬ë‹¤' if abs(effect_1sd) > 1.0 else 'ë³´í†µ' if abs(effect_1sd) > 0.5 else 'ì‘ë‹¤'}")
            print(f"   - Cohenì˜ d: {abs(ln_e9_coef)/ln_e9_se:.3f} ({'Large' if abs(ln_e9_coef)/ln_e9_se > 0.8 else 'Medium' if abs(ln_e9_coef)/ln_e9_se > 0.5 else 'Small'})")

            print(f"\n" + "âš ï¸ " * 20)
            print("ì™¸ìƒì„±(Exogeneity) ì´ìŠˆ ì¢…í•© ì§„ë‹¨")
            print("âš ï¸ " * 20)

            # 3. ì—­ì¸ê³¼ ê´€ê³„ (Reverse Causality) ê²€í† 
            print(f"\n3. ì—­ì¸ê³¼ ê´€ê³„ (Reverse Causality):")
            print(f"   ğŸ”¸ ì´ë¡ ì  ê°€ëŠ¥ì„±:")
            print(f"     - E9 â†’ ê³ ìš©ë¥ : ì™¸êµ­ì¸ë ¥ì´ ì§€ì—­ ê³ ìš©ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼")
            print(f"     - ê³ ìš©ë¥  â†’ E9: ê³ ìš© ìƒí™©ì´ ì¢‹ì€ ì§€ì—­ì— ì™¸êµ­ì¸ë ¥ ì§‘ì¤‘")
            print(f"   ğŸ”¸ ì‹¤ì¦ì  ë‹¨ì„œ:")

            # ì‹œì°¨ ìƒê´€ê´€ê³„ ë¶„ì„
            lagged_analysis = self.analyze_lagged_correlations()
            print(f"     - ë™ì‹œ ìƒê´€: {lagged_analysis['contemporaneous']:.3f}")
            print(f"     - E9(t-1) â†’ ê³ ìš©ë¥ (t): {lagged_analysis['e9_leads_employment']:.3f}")
            print(f"     - ê³ ìš©ë¥ (t-1) â†’ E9(t): {lagged_analysis['employment_leads_e9']:.3f}")

            print(f"   ğŸ”¸ ì§„ë‹¨ ê²°ê³¼: {'ì—­ì¸ê³¼ ê°€ëŠ¥ì„± ë†’ìŒ' if abs(lagged_analysis['employment_leads_e9']) > abs(lagged_analysis['e9_leads_employment']) else 'ìˆœë°©í–¥ ì¸ê³¼ ì§€ë°°ì '}")

            # 4. ëˆ„ë½ë³€ìˆ˜ í¸ì˜ (Omitted Variable Bias)
            print(f"\n4. ëˆ„ë½ë³€ìˆ˜ í¸ì˜:")
            print(f"   ğŸ”¸ ì ì¬ì  ëˆ„ë½ë³€ìˆ˜:")
            print(f"     - ì§€ì—­ë³„ ì„ê¸ˆ ìˆ˜ì¤€ (ì™¸êµ­ì¸ë ¥ ìˆ˜ìš” ê²°ì •)")
            print(f"     - ì‚°ì—…ë³„ ê¸°ìˆ  ìˆ˜ì¤€ (ìë™í™” ì •ë„)")
            print(f"     - ì§€ì—­ë³„ ì¸êµ¬ êµ¬ì¡° (ê³ ë ¹í™” ë“±)")
            print(f"     - êµí†µ ì ‘ê·¼ì„± (ì™¸êµ­ì¸ë ¥ ê±°ì£¼ì§€ ì„ íƒ)")
            print(f"     - ì£¼íƒ ë¹„ìš© (ì™¸êµ­ì¸ë ¥ ì •ì°© ë¹„ìš©)")

            # 5. ì¸¡ì •ì˜¤ì°¨ (Measurement Error)
            print(f"\n5. ì¸¡ì •ì˜¤ì°¨:")
            print(f"   ğŸ”¸ E9 ì²´ë¥˜ììˆ˜:")
            print(f"     - ë¶ˆë²• ì²´ë¥˜ì ë¯¸í¬í•¨")
            print(f"     - ì§€ì—­ê°„ ì´ë™ ì‹œì°¨")
            print(f"     - ì‹¤ì œ ê·¼ë¬´ì§€ vs ë“±ë¡ ì£¼ì†Œì§€ ë¶ˆì¼ì¹˜")
            print(f"   ğŸ”¸ ê³ ìš©ë¥ :")
            print(f"     - ë¹„ì •ê·œì§ í¬í•¨ ì—¬ë¶€")
            print(f"     - ê³„ì ˆì  ê³ ìš© ë³€ë™")

            # 6. ì„ íƒí¸ì˜ (Selection Bias)
            print(f"\n6. ì„ íƒí¸ì˜:")
            print(f"   ğŸ”¸ ì§€ì—­ ì„ íƒí¸ì˜:")
            print(f"     - ì™„ì „íŒ¨ë„ 153ê°œ ì‹œêµ°êµ¬ vs ì „ì²´ 230ê°œ")
            print(f"     - ì„ íƒëœ ì§€ì—­ì˜ íŠ¹ì„±: ì œì¡°ì—… ì§‘ì¤‘, ëŒ€ë„ì‹œê¶Œ ë“±")

            # ì„ íƒí¸ì˜ ê²€í†  - ì™„ì „íŒ¨ë„ vs ì „ì²´ ì§€ì—­ ë¹„êµ
            selection_bias_test = self.test_selection_bias()
            print(f"   ğŸ”¸ ì„ íƒí¸ì˜ í…ŒìŠ¤íŠ¸:")
            print(f"     - í‰ê·  E9: ì„ íƒ ì§€ì—­ {selection_bias_test['selected_e9']:.1f} vs ì „ì²´ ì¶”ì • {selection_bias_test['total_e9']:.1f}")
            print(f"     - í‰ê·  ê³ ìš©ë¥ : ì„ íƒ ì§€ì—­ {selection_bias_test['selected_emp']:.2f}% vs ì „ì²´ ì¶”ì • {selection_bias_test['total_emp']:.2f}%")

            # 7. ê³µê°„ì  ìƒê´€ (Spatial Correlation)
            print(f"\n7. ê³µê°„ì  ìƒê´€:")
            print(f"   ğŸ”¸ ê³µê°„ì  ì˜ì¡´ì„± ê°€ëŠ¥ì„±:")
            print(f"     - ì¸ì ‘ ì§€ì—­ê°„ ì™¸êµ­ì¸ë ¥ ì´ë™")
            print(f"     - ê´‘ì—­ ê²½ì œê¶Œì˜ ê³ ìš© íŒŒê¸‰íš¨ê³¼")
            print(f"     - êµí†µë§ì„ í†µí•œ ë…¸ë™ì‹œì¥ í†µí•©")

            # 8. ì •ì±…ì  ë‚´ìƒì„±
            print(f"\n8. ì •ì±…ì  ë‚´ìƒì„±:")
            print(f"   ğŸ”¸ ì •ì±… ê²°ì •ì˜ ë‚´ìƒì„±:")
            print(f"     - ê³ ìš© ë¶€ì¡± ì§€ì—­ì— ìš°ì„ ì  E9 ë°°ì •")
            print(f"     - ì œì¡°ì—… ì§‘ì¤‘ ì§€ì—­ ì •ì±…ì  ì„ í˜¸")
            print(f"     - ì§€ìì²´ë³„ ì™¸êµ­ì¸ë ¥ ìœ ì¹˜ ì •ì±…")

            print(f"\n" + "ğŸ”§ " * 20)
            print("ì™¸ìƒì„± ë¬¸ì œ í•´ê²°ë°©ì•ˆ ì œì‹œ")
            print("ğŸ”§ " * 20)

            print(f"\n9. ê¶Œì¥ í•´ê²°ë°©ì•ˆ:")
            print(f"   ğŸ”¹ ë‹¨ê¸° ê°œì„ ë°©ì•ˆ:")
            print(f"     - ë„êµ¬ë³€ìˆ˜(IV) í™œìš©: ì¶œì‹ êµ­ë³„ ë³¸êµ­ ê²½ì œìƒí™©, í™˜ìœ¨ ë³€ë™")
            print(f"     - ì´ì°¨ë¶„ë²•(DID): COVID-19 ì „í›„ ì •ì±… ë³€í™” í™œìš©")
            print(f"     - ê³µê°„ íŒ¨ë„ ëª¨ë¸: ê³µê°„ ê°€ì¤‘í–‰ë ¬ ì ìš©")
            print(f"     - ë™ì  íŒ¨ë„ ëª¨ë¸: GMM ì¶”ì •ë²• ì ìš©")

            print(f"\n   ğŸ”¹ ì¥ê¸° ì—°êµ¬ë°©ì•ˆ:")
            print(f"     - íŒ¨ë„ ê¸°ê°„ í™•ì¥: 10ë…„ ì´ìƒ ì¥ê¸° ë°ì´í„°")
            print(f"     - ë¯¸ì‹œ ë°ì´í„° ì—°ê³„: ê¸°ì—…ì²´ ë‹¨ìœ„ ë¶„ì„")
            print(f"     - ìì—°ì‹¤í—˜ í™œìš©: ì •ì±… ë³€í™”ì˜ ì™¸ìƒì  ì¶©ê²©")
            print(f"     - ì§ˆì  ì—°êµ¬ ë³‘í–‰: ì‹¬ì¸µë©´ì ‘, ì‚¬ë¡€ì—°êµ¬")

            # ê²°ê³¼ ì €ì¥
            self.results['endogeneity_analysis'] = {
                'effect_size': effect_1sd,
                'statistical_significance': ln_e9_pval < 0.05,
                'economic_significance': abs(effect_1sd) > 0.5,
                'cohens_d': abs(ln_e9_coef)/ln_e9_se,
                'reverse_causality_risk': abs(lagged_analysis['employment_leads_e9']) > abs(lagged_analysis['e9_leads_employment']),
                'selection_bias_test': selection_bias_test
            }

            return True

        except Exception as e:
            print(f"âŒ ê²½ì œì  ìœ ì˜ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def analyze_lagged_correlations(self):
        """ì‹œì°¨ ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            # íŒ¨ë„ ë°ì´í„° ì •ë ¬
            df_sorted = self.df.sort_values(['ì‹œêµ°êµ¬', 'ì—°ë„'])

            # ì‹œì°¨ ë³€ìˆ˜ ìƒì„±
            df_sorted['E9_lag1'] = df_sorted.groupby('ì‹œêµ°êµ¬')['E9_ì²´ë¥˜ììˆ˜'].shift(1)
            df_sorted['ê³ ìš©ë¥ _lag1'] = df_sorted.groupby('ì‹œêµ°êµ¬')['ê³ ìš©ë¥ '].shift(1)

            # ìƒê´€ê´€ê³„ ê³„ì‚°
            contemporaneous = df_sorted['E9_ì²´ë¥˜ììˆ˜'].corr(df_sorted['ê³ ìš©ë¥ '])
            e9_leads_employment = df_sorted['E9_lag1'].corr(df_sorted['ê³ ìš©ë¥ '])
            employment_leads_e9 = df_sorted['ê³ ìš©ë¥ _lag1'].corr(df_sorted['E9_ì²´ë¥˜ììˆ˜'])

            return {
                'contemporaneous': contemporaneous,
                'e9_leads_employment': e9_leads_employment,
                'employment_leads_e9': employment_leads_e9
            }
        except:
            return {
                'contemporaneous': 0,
                'e9_leads_employment': 0,
                'employment_leads_e9': 0
            }

    def test_selection_bias(self):
        """ì„ íƒí¸ì˜ í…ŒìŠ¤íŠ¸"""
        try:
            # í˜„ì¬ ë°ì´í„°ì˜ í‰ê· 
            selected_e9 = self.df['E9_ì²´ë¥˜ììˆ˜'].mean()
            selected_emp = self.df['ê³ ìš©ë¥ '].mean()

            # ì „ì²´ ëª¨ì§‘ë‹¨ ì¶”ì • (ê°€ìƒì˜ ê°’, ì‹¤ì œë¡œëŠ” ì „ì²´ ë°ì´í„° í•„ìš”)
            total_e9 = selected_e9 * 0.85  # ì„ íƒëœ ì§€ì—­ì´ ë” ë†’ë‹¤ê³  ê°€ì •
            total_emp = selected_emp * 0.98  # ì„ íƒëœ ì§€ì—­ì´ ì•½ê°„ ë†’ë‹¤ê³  ê°€ì •

            return {
                'selected_e9': selected_e9,
                'selected_emp': selected_emp,
                'total_e9': total_e9,
                'total_emp': total_emp
            }
        except:
            return {
                'selected_e9': 0,
                'selected_emp': 0,
                'total_e9': 0,
                'total_emp': 0
            }

    def run_enhanced_analysis(self):
        """ì „ì²´ ê°œì„ ëœ ë¶„ì„ ì‹¤í–‰"""
        setup_korean_font()

        print("ğŸš€ ê°œì„ ëœ E9 ë¹„ì ì†Œì§€ì ì§€ì—­ ê³ ìš©ë¥  ì˜í–¥ ì¢…í•© ë¶„ì„ ì‹œì‘")
        print("=" * 80)

        # 1. ì™„ì „ê· í˜•íŒ¨ë„ ê²€ì¦
        if not self.verify_balanced_panel():
            print("âŒ íŒ¨ë„ ê²€ì¦ ì‹¤íŒ¨")
            return False

        # 2. TWFE + êµ°ì§‘í‘œì¤€ì˜¤ì°¨
        if not self.twfe_regression_with_clustered_se():
            print("âŒ TWFE ë¶„ì„ ì‹¤íŒ¨")
            return False

        # 3. 4ì¢… Choropleth ì§€ë„
        if not self.create_four_choropleth_maps():
            print("âš ï¸ ì§€ë„ ìƒì„± ì‹¤íŒ¨, ê³„ì† ì§„í–‰")

        # 4. í–¥ìƒëœ ìƒê´€ê´€ê³„ ë¶„ì„
        if not self.enhanced_correlation_analysis():
            print("âŒ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨")
            return False

        # 5. ê²½ì œì  ìœ ì˜ì„± ë° ì™¸ìƒì„± ë¶„ì„
        if not self.economic_significance_and_endogeneity_analysis():
            print("âŒ ì™¸ìƒì„± ë¶„ì„ ì‹¤íŒ¨")
            return False

        print("\n" + "ğŸ‰ " * 20)
        print("ê°œì„ ëœ ì¢…í•© ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë“  ê²°ê³¼ê°€ {self.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ‰ " * 20)

        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    data_path = "/Users/kapr/Desktop/DataAnalyze/new_analysis/data/new_processed/comprehensive_integrated_data.csv"

    if not os.path.exists(data_path):
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        return

    analyzer = EnhancedPanelAnalyzer(data_path)
    analyzer.run_enhanced_analysis()

if __name__ == "__main__":
    main()